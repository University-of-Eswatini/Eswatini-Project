{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method of Least Squares Approximation\n",
    "\n",
    "The method of least squares approximation is used to fit data given in\n",
    "tabular for to a simple function (e.g straight line, general polynomial,\n",
    "exponential functions, etc). Type type of approximation is known as\n",
    "**discrete** least squares approximation. Least squares approximations\n",
    "can also be used to approximate a continuous complicated function\n",
    "expression using simple functions. In this case, the approximation is\n",
    "referred to as continuous least squares approximation.\n",
    "\n",
    "# Discrete Linear Least Squares Approximation\n",
    "\n",
    "In linear least squares approximation, the aim is to find a least\n",
    "squares line $y = a_1x + a_0$ that approximates the set of $m$ data\n",
    "points $(x_k,y_k)$ for $k = 1,2,\\ldots,m$ given in a table.\n",
    "\n",
    "`{admonition} definition The linear least squares method involves finding the best approximating line when the error involved is the sum of the squares of the differences between the $y-$values on the approximating line and the given $y-$values.`\n",
    "\n",
    "The general problem of fitting the best least squares line to a\n",
    "collection of $m$ data points $\\displaystyle \\{x_i,y_i \\}_{i = 1}^{m}$\n",
    "involves minimizing the **least squares error**\n",
    "\n",
    "\\$ E(a_0,a_1) = \\_{i = 1}<sup>{m}</sup>2 \\$\n",
    "\n",
    "To obtain the minimum, we must solve\n",
    "\n",
    "$\\frac{\\partial E}{\\partial a_0} = 0,\\;\\;\\;\\text{and}\\;\\;\\;\\frac{\\partial E}{\\partial a_1} = 0.$\n",
    "\n",
    "Applying the chain rule to evaluate the derivative gives,\n",
    "\n",
    "\\$\n",
    "\\$\n",
    "\n",
    "\\$\n",
    "\\$\n",
    "\n",
    "Rearranging equations () and () gives the normal equations\n",
    "\n",
    "\\`\\`\\`{admonition} definition \\$\n",
    "\\$ which can be written in matrix form as\n",
    "\n",
    "\\$\n",
    "\\$\n",
    "\n",
    "    $\\begin{bmatrix}\n",
    "    a_0\\\\a_1\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "    \\displaystyle \\sum_{i = 1}^{m}y_i\\\\\n",
    "    \\displaystyle \\sum_{i = 1}^{m}x_iy_i\n",
    "    \\end{bmatrix}$\n",
    "\n",
    "The solution of this matrix equation is \\$\n",
    "\\$ where\n",
    "$\\displaystyle{d = m\\left(\\sum_{i = 1}^m x_i^2 \\right) - \\left(\\sum_{i = 1}^m x_i \\right)^2}$\n",
    "\\`\\`\\`\n",
    "\n",
    "As a concrete example, consider the example below\n",
    "\n",
    "**example** Find the least squares line approximating the data given in\n",
    "the following table\n",
    "\n",
    "     x    1     2     3     4     5     6     7      8      9      10\n",
    "    ----- ----- ----- ----- ----- ----- ----- ------ ------ ------ ------\n",
    "    y   1.2   3.6   4.1   5.1   7.0   9.0   10.0   12.4   12.9   15.5\n",
    "\n",
    "    : Linear least squares example\n",
    "\n",
    "    Plot the original data points and the least squares line using a fine\n",
    "    set of grid points.\n",
    "\n",
    "**Solution:** To find the least squares line that approximates this\n",
    "data, we extend the table to include rows that give the sum and columns\n",
    "for the products $x_i\\cdot x_i$ and $x_i \\cdot y_i$ as shown in Table\n",
    "().\n",
    "\n",
    "| $x_i$ | $y_i$ | $x_i^2$ | $x_iy_i$ |\n",
    "|:-----:|:-----:|:-------:|:--------:|\n",
    "|   1   |  1.2  |    1    |   1.2    |\n",
    "|   2   |  3.6  |    4    |   7.2    |\n",
    "|   3   |  4.1  |    9    |   12.3   |\n",
    "|   4   |  5.1  |   16    |   20.4   |\n",
    "|   5   |  7.0  |   25    |   35.0   |\n",
    "|   6   |  9.0  |   36    |   54.0   |\n",
    "|   7   | 10.0  |   49    |   70.0   |\n",
    "|   8   | 12.4  |   64    |   99.2   |\n",
    "|   9   | 12.9  |   81    |  116.1   |\n",
    "|  10   | 15.5  |   100   |  155\\.   |\n",
    "\n",
    "$\\sum x_i = 55$ $\\sum y_i = 80.8$ $\\sum x_i^2 = 385$ $\\sum x_i y_i = 570.4$  \n",
    "**Solution**: Linear least squares example\n",
    "\n",
    "The least squares approximation leads to this system of two equations\n",
    "$\\begin{aligned}  10 a_0 + 55 a_1 & = 80.8\\\\  55 a_0 + 385 a_1 & = 570.4 \\end{aligned}$\n",
    "\n",
    "whose solution is $a_0 = -0.32$ and $a_1 = 1.527$. So\n",
    "$P_1(x) = 1.527x - 0.32$.\n",
    "\n",
    "Figure\n",
    "<a href=\"#figLLSA1\" data-reference-type=\"ref\" data-reference=\"figLLSA1\">1</a>\n",
    "is a plot of the given data and the linear least squares line.\n",
    "\n",
    "<span class=\"image\">Linear least squares example</span>\n",
    "\n",
    "# Discrete Polynomial Least Squares Approximation\n",
    "\n",
    "The method of least squares approximation of a set of data\n",
    "$\\{(x_i,y_i)| i = 1,2,\\ldots,m\\}$ can be extended to approximation using\n",
    "a general polynomial of degree $n < m - 1$\n",
    "$P_n(x) = a_nx^n + a_{n-1}x^{n-1} + \\cdots + a_1x + a_0 = \\sum_{k = 0}^na_kx^k.$\n",
    "The aim it to determine the parameters $a_0,a_1,a_2,\\ldots,a_n$ to\n",
    "minimize the least squares error\n",
    "\n",
    "$E(a_0,a_1,a_2,\\ldots,a_n) = \\sum_{i = 1}^m \\left[y_i - P_n(x_i) \\right]^2.$\n",
    "The minimum for $E$ is obtained by solving\n",
    "$\\displaystyle \\frac{\\partial E}{\\partial a_j}$, for each\n",
    "$j = 0,1,\\ldots,n$. That is, we must solve\n",
    "\n",
    "\\$ = 0,;; = 0,;; = 0,;; ,;; = 0. \\$\n",
    "\n",
    "Thus, we must have\n",
    "\n",
    "\\$\n",
    "\\$\n",
    "\n",
    "Rearranging gives,\n",
    "$\\sum_{i = 1}^m P_n(x_i) \\frac{\\partial P_n(x_i)}{\\partial a_j} =  \\sum_{i = 1}^m y_i \\frac{\\partial P_n(x_i)}{\\partial a_j}$\n",
    "for $j = 0,1,2,\\ldots,n$.\n",
    "\n",
    "Given that $\\displaystyle P_n(x_i) = \\sum_{k = 0}^n a_k x_i^k$, we have\n",
    "\n",
    "$\\frac{\\partial P_n(x_i)}{\\partial a_j} = \\frac{\\partial}{\\partial a_j}\\left( \\sum_{k = 0}^n a_k x_i^k\\right) = x_i^j.$\n",
    "Thus, the normal equations are given as\n",
    "$\\sum_{i = 1}^m \\sum_{k = 0}^n a_k x_i^k x_i^j = \\sum_{i = 1}^m y_i x_i^j$\n",
    "or, in another form,\n",
    "$\\sum_{k = 0}^n a_k \\left(\\sum_{i = 1}^m x_i^{j+k} \\right) = \\sum_{i = 1}^m y_i x_i^j$\n",
    "for $j = 0,1,2,\\ldots,n$. It is helpful to write the normal equations in\n",
    "expandable form as\n",
    "\n",
    "\\$\n",
    "\\\\ \\$\n",
    "\n",
    "In matrix form, the normal equations can be written as\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "    \\sum x_i^0 & \\sum x_i^1 & \\sum x_i^2 & \\cdots & \\sum x_i^n \\\\\n",
    "    \\sum x_i^1 & \\sum x_i^2 & \\sum x_i^3 & \\cdots & \\sum x_i^{n+1} \\\\\n",
    "    \\sum x_i^2 & \\sum x_i^3 & \\sum x_i^4 & \\cdots & \\sum x_i^{n+2} \\\\\n",
    "    \\vdots   &    \\vdots  &    \\vdots  &  \\vdots& \\vdots \\\\ \n",
    "    \\sum x_i^n & \\sum x_i^{n+1} & \\sum x_i^{n+2} & \\cdots & \\sum x_i^{2n} \\\\\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    a_0\\\\a_1\\\\a_2\\\\\\vdots\\\\a_n\n",
    "    \\end{bmatrix}\n",
    "    = \n",
    "    \\begin{bmatrix}\n",
    "    \\sum y_i x_i^0\\\\\n",
    "    \\sum y_i x_i^1\\\\\n",
    "    \\sum y_i x_i^2\\\\\n",
    "        \\vdots \\\\\n",
    "    \\sum y_i x_i^n\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "example Find the quadratic polynomial that best fits the following data\n",
    "in the sense of least squares\n",
    "\n",
    "| $x$ | -2  | -1  | 0   | 1   | 2   |\n",
    "|:----|:----|:----|:----|:----|:----|\n",
    "| $y$ | 2   | 1   | 1   | 1   | 2   |\n",
    "\n",
    "Quadratic least squares example\n",
    "\n",
    "**Solution:** We rearrange the table as follows\n",
    "\n",
    "| $x_i$          |     $y_i$      |      $x_i^2$      | $x_i^3$          |      $x_i^4$      |      $x_iy_i$      |      $x_i^2y_i$       |\n",
    "|:---------------|:--------------:|:-----------------:|:-----------------|:-----------------:|:------------------:|:---------------------:|\n",
    "| -2             |       2        |         4         | -8               |        16         |         -4         |           8           |\n",
    "| -1             |       1        |         1         | -1               |         1         |         -1         |           1           |\n",
    "| 0              |       1        |         0         | 0                |         0         |         0          |           0           |\n",
    "| 1              |       1        |         1         | 1                |         1         |         1          |           1           |\n",
    "| 2              |       2        |         4         | 8                |        16         |         4          |           8           |\n",
    "| $\\sum x_i = 0$ | $\\sum y_i = 7$ | $\\sum x_i^2 = 10$ | $\\sum x_i^3 = 0$ | $\\sum x_i^4 = 34$ | $\\sum x_i y_i = 0$ | $\\sum x_i^2 y_i = 18$ |\n",
    "\n",
    "Solution: Quadratic least squares example\n",
    "\n",
    "For this problem, the normal equations are\n",
    "\n",
    "\\$\n",
    "\\$ Evaluating the sums gives\n",
    "$\\begin{array}{cccccccc} 5a_0 & + & & & 10 a_2 & = & 7 \\\\  & &10a_1& & & =& 0 \\\\ 10a_0&+&& &34a_2&=&18 \\end{array}$\n",
    "Solving the above system gives\n",
    "$a_0 = \\frac{29}{35},\\;\\;\\;a_1 = 0,\\;\\;\\; a_2 = \\frac{2}{7}.$ Thus, the\n",
    "least squares polynomial of degree 2 that fits the data is\n",
    "$P_2(x) = \\frac{2}{7}x^2 + \\frac{29}{35}$ The total error is\n",
    "$E = \\sum_{i = 1}^{5}\\left[y_i - P(x_i)\\right]^2 = \\frac{2}{35} = 0.0571429$\n",
    "This is the least that can be obtained by using a polynomial of degree\n",
    "at most 2.\n",
    "\n",
    "# Non-polynomial least squares approximation\n",
    "\n",
    "The least squares approximation method is not limited to approximating\n",
    "using polynomials. Occasionally, it can be assumed that the data are\n",
    "exponentially related. This requires the approximating function to be of\n",
    "the form $y = b e^{ax}\\;\\;\\;\\; \\text{or}\\;\\;\\;\\; y = bx^a.$ The\n",
    "corresponding least square error to be minimized is\n",
    "\n",
    "$E(a,b) = \\sum_{i = 1}^m (y_i - b e^{ax_i})^2\\;\\;\\;\\; \\text{or}\\;\\;\\;\\; E(a,b) = \\sum_{i = 1}^m (y_i - b x_i^a)^2$\n",
    "To avoid ending up with nonlinear equations that cannot be solved\n",
    "exactly, it may be necessary to simplify the approximating equation\n",
    "using properties of logarithms\n",
    "$\\ln y = \\ln b + ax\\;\\;\\; \\text{or}\\;\\;\\; \\ln y = \\ln b + a \\ln x$\n",
    "\n",
    "Another commonly used function is the combination between logarithmic,\n",
    "trigonometric and exponential functions $y = a \\ln x + b \\cos x + c e^x$\n",
    "with least squares error\n",
    "\n",
    "$E(a,b,c) = \\sum_{i = 1}^m \\left[y_i - (a\\ln x_i + b \\cos x_i + ce^{x_i}) \\right]^2$\n",
    "\n",
    "# Discrete Least Squares Using Basis Functions\n",
    "\n",
    "The least squares principle can be extended general functions in exactly\n",
    "the same way as was done in previous sections. Suppose that data points\n",
    "conform to the general relationship\n",
    "\n",
    "\\$ y = \\_{k = 0}^{n} c_k \\_k(x) \\$ in which the **basis functions**\n",
    "$\\phi_0, \\phi_1,\\ldots, \\phi_n$ are known and held fixed. The\n",
    "coefficients $c_0,c_1,c_2,\\ldots,c_n$ are determined according to the\n",
    "principle of least squares in the usual way. Proceeding as before, the\n",
    "least squares error expression is\n",
    "\n",
    "\\$ E(c_0,c_1,,c_n) = \\_{j = 1}<sup>{m}</sup>2\\$\n",
    "\n",
    "It can be shown that the normal equations in this situation are given by\n",
    "\n",
    "$$\n",
    " \\sum_{k = 0}^n \\left[\\sum_{j=1}^m \\phi_i(x_j) \\phi_{k}(x_j)\\right]c_k = \\sum_{j = 1}^m y_j \\phi_i(x_j),\\;\\;\\;\\text{where $0 \\leq i \\leq n$}\n",
    " $$\n",
    "\n",
    "Equation\n",
    "<a href=\"#LSABasis3\" data-reference-type=\"eqref\" data-reference=\"LSABasis3\">[LSABasis3]</a>\n",
    "forms a linear matrix system that depends on the nature of the basis\n",
    "$\\{\\phi_0(x),\\phi_1(x),\\ldots,\\phi_n(x)\\}$. The resulting matrix system\n",
    "has an identity matrix as coefficient matrix if the basis has the\n",
    "property of orthonormality.\n",
    "\n",
    "`{admonition} definition The basis $\\{\\phi_0(x),\\phi_1(x),\\ldots,\\phi_n(x)\\}$ is said to be an orthornomal set of functions if $\\sum_{j=1}^m \\phi_i(x_j) \\phi_{k}(x_j)  =  \\left\\{\\begin{array}{cc} 1~~&~~i  =  k\\\\ 0~~&~~i \\neq k  \\end{array} \\right.$`\n",
    "\n",
    "Applying the orthonormality property on equation\n",
    "<a href=\"#LSABasis3\" data-reference-type=\"eqref\" data-reference=\"LSABasis3\">[LSABasis3]</a>\n",
    "simplifies the equation to\n",
    "\n",
    "{Orthonormal2}\n",
    "$$c_i = \\sum_{j = 1}^{m}y_j\\phi_i(x_j),\\;\\;\\;\\;\\; 0 \\leq i \\leq n$$\n",
    "\n",
    "Equation\n",
    "<a href=\"#Orthonormal2\" data-reference-type=\"eqref\" data-reference=\"Orthonormal2\">[Orthonormal2]</a>\n",
    "is an explicit formula for evaluating the coefficients $c_i$.\n",
    "\n",
    "## Summary:Discrete Least Squares Approximation\n",
    "\n",
    "**Keywords:** Minimax Problem, Absolute Deviation, Least Squares, Normal\n",
    "equations\n",
    "\n",
    "**Intended learning outcomes (ILOs):**\n",
    "\n",
    "1.  **Discrete Least squares approximation**\n",
    "\n",
    "    -   Derive the normal equations based on minimizing the linear least\n",
    "        squares error\n",
    "        $E(a_0,a_1) = \\sum_{i=1}^{m} \\left[y_i - (a_1 x + a_0) \\right]^2$\n",
    "\n",
    "    -   Compute the least squares line approximating the data from a\n",
    "        given table.\n",
    "\n",
    "    -   Derive the normal equations based on minimizing the polynomial\n",
    "        least squares error\n",
    "        $E(a_0,a_1,a_2,\\ldots,a_n) = \\sum_{i=1}^{m} \\left[y_i - P_n(x_i) \\right]^2$\n",
    "        where $\\displaystyle { P_n(x) = \\sum_{k=0}^n a_k x^k }$.\n",
    "\n",
    "    -   Find data given in a table with discrete least squares\n",
    "        polynomials of degree at most $n = 2,3,4,\\ldots$.\n",
    "\n",
    "    -   *Use Matlab to compute the coefficients,\n",
    "        $a_0,a_1,a_2,\\ldots,a_n$, of the discrete least squares\n",
    "        polynomials.*\n",
    "\n",
    "## Practice Problems\n",
    "\n",
    "1.  Find an equation of the form $y = ae^{x^2} + bx^3$ that best fits\n",
    "    the points (-1,0), (0,1) and (1,2) in the least-squares sense.\n",
    "\n",
    "2.  Find the equation of a parabola of the form $y = ax^2 + b$ that best\n",
    "    represents the following data using the method of least squares.\n",
    "\n",
    "    center $x$ -1 0 1 —– —– —– —– $y$ 3.1 0.9 2.9\n",
    "\n",
    "3.  What constant $c$ makes the expression\n",
    "    $\\sum_{k = 0}^m [f(x_k) - c e^{x_k}]^2$ as small as possible?\n",
    "\n",
    "4.  Show that the formula for the best line to fit data $(k,y_k)$ at the\n",
    "    integers $k$ for $1 \\leq k \\leq n$ is $y = ax + b$ where\n",
    "\n",
    "5.  Find the best function (in the least squares sense) that fits the\n",
    "    following data points and is of the form\n",
    "    $f(x) = a \\sin \\pi x + b \\cos \\pi x$\n",
    "\n",
    "    center $x$ -1 $-\\frac{1}{2}$ 0 $\\frac{1}{2}$ 1 —– —- —————- — —————\n",
    "    — $y$ -1 0 1 2 1\n",
    "\n",
    "6.  Find the quadratic polynomial that best fits the following data in\n",
    "    the sense of least squares\n",
    "\n",
    "    center $x$ -2     -1     0     1     2   —— ——– ——– ——- ——- ——- $y$\n",
    "    2 1 1 1 2\n",
    "\n",
    "7.  Find the least squares polynomials of degrees 1,2 and 3 for the data\n",
    "    in the following tables. Compute the error $E$ in each case. Graph\n",
    "    the data and the polynomials.\n",
    "\n",
    "    1.  center $x$ 0 0.15 0.31 0.5 0.6 0.75 —– —– ——- ——- ——- ——- ——-\n",
    "        $y$ 1.0 1.004 1.031 1.117 1.223 1.422\n",
    "\n",
    "    2.  center $x$ 1.0   1.1 1.3 1.5 1.9 2.1 —– ——- —— —— —— —— —— $y$\n",
    "        1.84 1.96 2.21 2.45 2.94 3.18\n",
    "\n",
    "# Continuous Least Squares Approximation\n",
    "\n",
    "In the last section we discussed Discrete Least Squares Approximation in\n",
    "which a collection of data points were fitted into a continuous\n",
    "function. In this section, we consider the approximation of a continuous\n",
    "function by another continuous function (e.g a polynomial function).\n",
    "\n",
    "Suppose $f \\in C[a,b]$ and that a polynomial\n",
    "$\\displaystyle P_n(x) = \\sum_{k=0}^n a_k x^k$ is required to minimize\n",
    "the error\n",
    "\n",
    "\\$ \\_a^b ^2\\~dx. \\$ The above expression denotes the total square of the\n",
    "deviation (error) of $P_n(x)$ from $f(x)$. For continuous least squares\n",
    "approximation using polynomials, the aim is to determine the\n",
    "coefficients $a_0,a_1,\\ldots,a_n$ that will minimize the error $E$\n",
    "defined as\n",
    "$E(a_0,a_1,\\ldots,a_n) = \\int_a^b \\left( f(x) - \\sum_{k=0}^n a_k x^k \\right)^2~ dx$\n",
    "\n",
    "The necessary condition for the coefficients to minimize $E$ is that\n",
    "$\\frac{\\partial E}{\\partial a_j} = 0,\\;\\;\\;\\;\\;\\text{for each}\\;\\; j=0,1,2,\\ldots,n.$\n",
    "Differentiating the integral gives\n",
    "\n",
    "\\$\n",
    "\\$ Hence, to determine $a_0,a_1,\\ldots,a_n$, and the polynomial\n",
    "$P_n(x)$, the **normal equations** given by\n",
    "<a href=\"#CLSAnormal\" data-reference-type=\"eqref\" data-reference=\"CLSAnormal\">[CLSAnormal]</a>\n",
    "must be solved.\n",
    "\n",
    "example Find the linear least squares approximation to $f(x) = e^x$ on\n",
    "\\[-1,1\\] and determine the relative error at $x = 0.5$.\n",
    "\n",
    "**Solution:** For this example we have $n = 1$ and\n",
    "$P_1(x) = a_0 + a_1x$. The normal equations are\n",
    "$\\begin{aligned} \\sum_{k = 0}^1 a_k \\int_{-1}^{1} x^{k+j}~dx = \\int_{-1}^{1} x^j f(x)~dx,\\;\\;\\;j = 0,1\\end{aligned}$\n",
    "Evaluating the summations gives\n",
    "$\\begin{aligned} a_0\\int_{-1}^{1} x^0 dx + a_1 \\int_{-1}^{1} x dx & = \\int_{-1}^{1} x^0 e^x dx,\\\\ a_0\\int_{-1}^{1} x^1 dx + a_1 \\int_{-1}^{1} x^2 dx & = \\int_{-1}^{1} x^1 e^x dx,\\\\\\end{aligned}$\n",
    "Evaluating the integrals gives\n",
    "$\\begin{aligned} 2a_0& = e - e^{-1},\\\\ \\frac{2}{3}a_1 & = 2e^{-1}\\end{aligned}$\n",
    "Thus,\n",
    "$\\begin{aligned} a_0 &= \\frac{1}{2}(e - e^{-1}) = 1.1752\\\\ a_1 &= 3e^{-1} = 1.1037\\\n",
    "\\text{Hence};\\;\\;\\;P_1(x) &= 1.1752 + 1.1037x \\end{aligned}$ At\n",
    "$x = 0.5$, $P_1(0.5) = 1.7271$ (approximation) and\n",
    "$f(0.5) = e^{0.5} = 1.6487$ (exact value). Thus, the relative error is\n",
    "\n",
    "$\\frac{|f(0.5) - P_1(0.5)|}{|f(0.5)|} = 0.0476$\n",
    "\n",
    "**example** Find the least squares approximation of degree 2 for the\n",
    "function $f(x) = e^x$ on \\[-1,1\\].\n",
    "\n",
    "**Solution:** The normal equations for $n = 2$,\n",
    "$P_2(x) = a_0 + a_1x + a_2x^2$ are\n",
    "\n",
    "\\$\\$\n",
    "    \\begin{bmatrix}\n",
    "    a_0\\\\a_1\\\\a_2\n",
    "    \\end{bmatrix}\n",
    "    = \n",
    "    \\begin{bmatrix}\n",
    "    \\int_{-1}^{1} x^0e^x\\\\\n",
    "    \\int_{-1}^{1} x^1e^x\\\\\n",
    "    \\int_{-1}^{1} x^2e^x\n",
    "    \\end{bmatrix}\n",
    "\n",
    "\\$\\$ Performing the integration yields\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & \\frac{2}{3}\\\\\n",
    "0 & \\frac{2}{3}& 0 \\\\\n",
    "\\frac{2}{3}& 0& \\frac{2}{5}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "a_0 \\\\a_1\\\\a_2\n",
    "\\end{bmatrix} \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "e - e^{-1}\\\\ 2e^{-1} \\\\ e - 5e^{-1}\n",
    "\\end{bmatrix} =\n",
    "$$ Solving gives $a_0 = 0.9963,\\;\\;\\;a_1 = 1.1037,\\;\\;\\;a_2 = 0.5368$\n",
    "Thus, $P_2(x) = 0.9963 + 1.1037x + 0.5368x^2$\n",
    "\n",
    "example Find the least squares quadratic function of the form\n",
    "$ax^2 + b$, which best fits the curve $y = \\sqrt{2x +1}$ over the\n",
    "interval $0 \\leq x \\leq \\frac{3}{2}$.\n",
    "\n",
    "**Solution:** Define the least squares error as\n",
    "$E(a,b) = \\int_{0}^{\\frac{3}{2}} \\left[ y - (ax^2+b)\\right]^2dx$ To\n",
    "minimize the least squares error, we differentiate $E(a,b)$ with respect\n",
    "to $a$ and $b$, in turn, and equate the derivative to zero.\n",
    "\n",
    "\\$\n",
    "\\$\n",
    "\n",
    "\\$\n",
    "\\$\n",
    "\n",
    "Thus, the normal equations are \\$\n",
    "\\$\n",
    "\n",
    "To compute the integral\n",
    "$\\displaystyle I_2 = \\int_{0}^{\\frac{3}{2}} x^2\\sqrt{2x+1}~dx$ we set\n",
    "$u = 2x + 1$ and note that $dx = \\frac{1}{2}du$. Also, when $x = 0$,\n",
    "$u = 1$ and when $x = \\frac{3}{2}$, $u = 4$. Thus,\n",
    "\n",
    "\\$\\$\n",
    "\\$\\$ Substituting in the normal equations gives  \n",
    "\\$\n",
    "\\$ Thus,\n",
    "\n",
    "\\$\n",
    "\\$ Thus, the least squares approximating polynomial is\n",
    "$\\displaystyle y = \\frac{226}{567}x^2 + \\frac{475}{378}$\n",
    "\n",
    "## Summary:Continuous Least squares approximation\n",
    "\n",
    "1.  **Intended learning outcomes (ILOs):**\n",
    "\n",
    "    -   Derive the normal equations based on minimizing the least\n",
    "        squares error\n",
    "\n",
    "        $E(a_0,a_1,a_2,\\ldots,a_n) = \\int_a^b \\left[f(x) - P_n(x) \\right]^2dx$\n",
    "        where $\\displaystyle { P_n(x) = \\sum_{k=0}^n a_k x^k }$.\n",
    "\n",
    "    -   Compute the least squares approximating polynomial of degree\n",
    "        $n = 2,3,\\ldots$, for a given function $f(x)$ on a given\n",
    "        interval $[a,b]$\n",
    "\n",
    "    -   *Use Matlab to compute the coefficients,\n",
    "        $a_0,a_1,a_2,\\ldots,a_n$, of the continuous least squares\n",
    "        polynomials.*\n",
    "\n",
    "## Practice Problems\n",
    "\n",
    "1.  Find the linear least squares approximation to $f(x) = x^2 + 3x + 2$\n",
    "    on \\[0,1\\].\n",
    "\n",
    "2.  Find the least squares quadratic approximation to\n",
    "    $\\displaystyle \\frac{1}{x}$ on \\[1,4\\]\n",
    "\n",
    "3.  Find the least squares approximations to $\\displaystyle \\sqrt{x}$ on\n",
    "    \\[0,1\\] of degrees 1, 2 and 3.\n",
    "\n",
    "4.  Find the least squares approximating polynomial of degree 2 to the\n",
    "    following functions on the given intervals\n",
    "\n",
    "    1.  $\\displaystyle f(x) = e^x$ on \\[0,2\\]\n",
    "\n",
    "    2.  $\\displaystyle f(x) = \\frac{1}{x+2}$ on \\[-1,1\\]\n",
    "\n",
    "5.  Find the least squares approximating polynomial of degree 3 to the\n",
    "    following functions on the interval $[-1,1]$\n",
    "\n",
    "    1.  $\\displaystyle f(x) = e^x$\n",
    "\n",
    "    2.  $\\displaystyle f(x) = \\cos(\\pi x)$\n",
    "\n",
    "# Continuous Least Squares Approximation using Orthogonal Basis Functions\n",
    "\n",
    "In the previous section, we considered least squares approximation using\n",
    "a general $n$-th degree polynomial of the form\n",
    "$P_n(x) = a_0 + a_1x + a_2x^2 + \\cdots + a_{n-1}x^{n-1} + a_nx^n = \\sum_{k = 0}^{n} a_k x^k$\n",
    "which is a linear combination of the *basis functions*\n",
    "$\\{1,x,x^2,\\ldots,x^{n-1},x^n \\}.$\n",
    "\n",
    "The disadvantage of using a polynomial constructed using monomial basis\n",
    "in least squares approximation is that the coefficient matrix in the\n",
    "linear system used to determine the unknown coefficients\n",
    "$a_0,a_1,\\ldots,a_n$ is ill-conditioned and leads to round-off error\n",
    "problems. The entries of the coefficient matrix are obtained from the\n",
    "integral \\$ \\_{a}^b x^{j+k}\\~dx = . \\$ The matrix is known as the\n",
    "**Hilbert matrix**. Linear systems involving the Hilbert matrix cannot\n",
    "be computed efficiently and are known to be susceptible to errors such\n",
    "as round off errors.\n",
    "\n",
    "In this section, we consider a computationally efficient way of\n",
    "obtaining the coefficients used to determine $P_n(x)$ in least squares\n",
    "approximation. We also discuss the use of **orthogonal** polynomials is\n",
    "dealing with the difficulties that arise in solving large,\n",
    "ill-conditioned systems of linear equations. To facilitate the\n",
    "discussions we present some new concepts and definitions.\n",
    "\n",
    "`{admonition} definition The set of functions $\\{\\phi_0,\\phi_1,\\phi_2,\\ldots,\\phi_n \\}$ is said to be **linearly independent** on $[a,b]$ if, whenever $c_0\\phi_0(x) + c_1\\phi_1(x) + \\cdots + c_n\\phi_n(x)=0,\\;\\;\\;\\text{for all}\\;\\; x \\in [a,b]$ we have $c_0 = c_1 = \\cdots = c_n = 0.$ Otherwise the set of functions is said to be **linearly dependent.**`\n",
    "\n",
    "theorem Suppose that $\\phi_j(x)$ is a polynomial of degree $j$ for each\n",
    "$j = 0,1,2,\\ldots,n$. Then the set $\\{\\phi_0,\\phi_1,\\ldots,\\phi_n \\}$ is\n",
    "linearly independent on any interval $[a,b]$.\n",
    "\n",
    "`{admonition} definition An integrable function $w(x)$ is said to be a weight function on the interval $I = [a,b]$ if $w(x) \\geq 0$, for all $x$ in $I$, but $w(x)\\neq 0$ on any subinterval of $I$.`\n",
    "\n",
    "The purpose of a weight function is to give more \"weight\" or influence\n",
    "to approximations on certain portions of the interval.\n",
    "\n",
    "Given that\n",
    "\n",
    "1.  $\\{\\phi_0,\\phi_1,\\ldots,\\phi_n \\}$ is a set of linearly independent\n",
    "    functions on $[a,b]$\n",
    "\n",
    "2.  $w(x)$ is a weight function for $[a,b]$.\n",
    "\n",
    "3.  $f(x)$ is continuous on $[a,b]$\n",
    "\n",
    "We seek a linear combination $P(x) = \\sum_{k=0}^n a_k \\phi_k(x)$ to\n",
    "minimize the error\n",
    "$E(a_0,a_1,\\ldots,a_n) = \\int_a^b w(x)\\left[f(x) - \\sum_{k=0}^n a_k \\phi_k(x) \\right]^2~dx$\n",
    "\n",
    "It should be noted that when $w(x) = 1$ and $\\phi_k(x) = x^k$, the\n",
    "problem reduces to the continuous least squares approximation using\n",
    "monomials that was discussed earlier.\n",
    "\n",
    "The normal equations are obtained by solving\n",
    "$\\displaystyle{\\frac{\\partial E}{\\partial a_j} = 0}$ for each\n",
    "$j = 0,1,\\ldots,n$. Thus, we have\n",
    "\n",
    "$\\frac{\\partial E}{\\partial a_j} = 2\\int_{a}^bw(x)\\left[f(x) - \\sum_{k=0}^n a_k \\phi_k(x) \\right]\\phi_j(x)~dx.$\n",
    "The **normal equations** can be written as\n",
    "${NormalOrthogonal} \\int_a^b w(x) f(x)\\phi_j(x)~dx = \\sum_{k=0}^{n}a_k \\int_a^b w(x)\\phi_k(x)\\phi_j(x)~dx,\\;\\;\\;\\;j = 0,1,2,\\ldots,n.$\n",
    "Suppose that the functions $\\phi_i$, for $i = 0,1,\\ldots,n$, are chosen\n",
    "in such a way that $$\n",
    "    \\int_a^b w(x)\\phi_k(x)\\phi_j(x)~dx = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "    0, &\\text{when $j \\neq k$},\\\\\n",
    "    \\alpha_j > 0,& \\text{when $j = k$}\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "then, for each $j = 0,1,\\ldots,n$, the normal equations will reduce to\n",
    "\n",
    "$\\int_a^b w(x) f(x)\\phi_j(x)~dx = a_j \\int_a^b w(x) [\\phi_j(x)]^2 = a_j\\alpha_j$\n",
    "and $a_j = \\frac{1}{\\alpha_j}\\int_a^b w(x)f(x)\\phi_j(x)~dx$ Hence the\n",
    "least squares approximation problem is greatly simplified when the\n",
    "functions $\\phi_0,\\phi_1,\\ldots,\\phi_n$ are chosen to satisfy condition\n",
    "reference=“orthogonality1”}, known as the **orthogonality condition.**\n",
    "\n",
    "`{admonition} definition The set $\\{\\phi_0,\\phi_1,\\ldots,\\phi_n \\}$ is said to be an **orthogonal set of functions** for the interval $[a,b]$ with respect to the weight function $w(x)$ if`\n",
    "$$\n",
    "\\int_a^b w(x)\\phi_k(x)\\phi_j(x)~dx = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "0, &\\text{when $j \\neq k$},\\\\\n",
    "\\alpha_j > 0,& \\text{when $j = k$}\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "If, in addition, $\\alpha_j = 1$ for each $j = 0,1,\\ldots,n$, the set is\n",
    "said to be **orthonormal**.\n",
    "\n",
    "theorem If $\\{\\phi_0,\\phi_1,\\ldots,\\phi_n\\}$ is an orthogonal set of\n",
    "functions on an interval $[a,b]$ with respect to the weight function\n",
    "$w(x)$, then the least squares approximation to $f(x)$ on $[a,b]$ with\n",
    "respect to $w(x)$ is $P_n(x) = \\sum_{j = 0}^{n}a_j\\phi_j(x)$ where, for\n",
    "each $j = 0,1,\\ldots,n$,\n",
    "$a_j = \\frac{\\int_a^bw(x)\\phi_j(x)f(x)~dx}{\\int_a^b w(x)[\\phi_j(x)]^2} = \\frac{1}{\\alpha_j}\\int_a^b w(x)\\phi_j(x)f(x)~dx$\n",
    "\n",
    "The following theorem, called the **Gram-Schmidt process**, can be used\n",
    "to recursively generate an orthogonal set of polynomials.\n",
    "\n",
    "theorem The set of polynomials $\\{\\phi_0,\\phi_1,\\ldots,\\phi_n \\}$\n",
    "defined in the following way is orthogonal on $[a,b]$ with respect to\n",
    "the weight function $w(x)$,\n",
    "$\\phi_0(x) = 1,\\;\\;\\; \\phi_1(x) = x - B_1,\\;\\;\\; \\text{for each}\\;\\;x\\in [a,b]$\n",
    "where,\n",
    "$B_1 = \\frac{\\int_{a}^b x w(x)[\\phi_0(x)]^2~dx}{\\int_a^b w(x)[\\phi_0(x)]^2~dx}$\n",
    "and when $k \\geq 2$,\n",
    "$\\phi_k(x) = (x-B_k)\\phi_{k-1}(x) - C_k\\phi_{k-2}(x),\\;\\;\\text{for each}\\;\\;x\\in [a,b]$\n",
    "where\n",
    "$B_k = \\frac{\\int_a^b xw(x)[\\phi_{k-1}(x)]^2~dx}{\\int_a^b w(x)[\\phi_{k-1}(x)]^2~dx}$\n",
    "and\n",
    "$C_k = \\frac{\\int_a^b xw(x)\\phi_{k-1}(x)\\phi_{k-2}(x)~dx}{\\int_a^b w(x)[\\phi_{k-2}(x)]^2~dx}$\n",
    "\n",
    "example 1. Use the Gram-Schmidt process to construct the Legendre\n",
    "polynomials $\\phi_1(x), \\phi_2(x), \\phi_3(x), \\phi_4(x)$. Here,\n",
    "$\\{\\phi_0,\\phi_1,\\phi_2,\\phi_3,\\phi_4\\}$ is an orthogonal set on\n",
    "$[-1,1]$ with respect to the weight $w(x) = 1$, given that\n",
    "$\\phi_0(x) = 1$.\n",
    "\n",
    "1.  Use the Legendre polynomials obtained in (i) above to approximate\n",
    "    $f(x) = |x|$ using on the interval $[-1,1]$.\n",
    "\n",
    "**Solution**  \n",
    "The orthogonal functions will be given by $$\n",
    "    \\begin{aligned}\n",
    "    \\phi_1(x) &= x - B_1\\\\\n",
    "    \\phi_2(x) &= (x - B_2)\\phi_1(x) - C_2\\phi_0(x)\\\\\n",
    "    \\phi_3(x) &= (x - B_3)\\phi_2(x) - C_3\\phi_1(x)\\\\\n",
    "    \\phi_4(x) &= (x - B_4)\\phi_3(x) - C_4\\phi_2(x)\\end{aligned}\n",
    "$$ Since\n",
    "$B_1 = \\frac{\\displaystyle \\int_{-1}^{1}xdx}{\\displaystyle \\int_{-1}^{1}dx} = 0$\n",
    "then $\\phi_1(x) = x$. The values of $B_2$ and $C_2$ are given by\n",
    "\n",
    "$B_2 = \\frac{\\displaystyle \\int_{-1}^{1}x[\\phi_1(x)]^2dx}{\\displaystyle \\int_{-1}^{1}[\\phi_1(x)]^2dx} = \\frac{\\displaystyle \\int_{-1}^{1}x^3dx}{\\displaystyle \\int_{-1}^{1}x^2dx} = 0$\n",
    "$c_2 =\\frac{\\displaystyle \\int_{-1}^{1}x\\phi_1(x)\\phi_0(x)dx}{\\displaystyle \\int_{-1}^{1}[\\phi_0(x)]^2dx} = \\frac{\\displaystyle \\int_{-1}^{1}x^2dx}{\\displaystyle \\int_{-1}^{1}dx} =\\frac{1}{3}$\n",
    "\n",
    "Therefore,\n",
    "$\\phi_2(x) = (x - B_2)\\phi_1(x) - C_2\\phi_0(x) = x^2 - \\frac{1}{3}$\n",
    "\n",
    "\\$ B_3 = = = 0 \\$ and\n",
    "\n",
    "\\$\n",
    "\\$ Therefore,\n",
    "$\\displaystyle \\phi_3(x) = (x - B_3)\\phi_2(x) - C_3\\phi_1(x) = x^3 - \\frac{3}{5}x$.\n",
    "\n",
    "Lastly, we have\n",
    "\n",
    "$B_4 = \\frac{\\displaystyle \\int_{-1}^{1}x[\\phi_3(x)]^2dx}{\\displaystyle \\int_{-1}^{1}[\\phi_3(x)]^2dx} = \\frac{\\displaystyle \\int_{-1}^{1}x\\left(x^3 - \\frac{3}{5}x\\right)^2dx}{\\displaystyle \\int_{-1}^{1}\\left(x^3 - \\frac{3}{5}x\\right)^2dx} = 0$\n",
    "and $$\n",
    " \\begin{aligned}\n",
    "C_4&=\\displaystyle \\frac{\\displaystyle \\int_{-1}^{1}x\\phi_3(x)\\phi_2(x)dx}{\\displaystyle \\int_{-1}^{1}[\\phi_2(x)]^2dx}\n",
    "=\\displaystyle  \\frac{\\displaystyle \\displaystyle \\int_{-1}^{1}x\\left(x^3 - \\frac{3}{5}x\\right)\\left(x^2 - \\frac{1}{3} \\right)dx}{\\displaystyle \\int_{-1}^{1}\\left(x^2 - \\frac{1}{3}\\right)^2dx} =\n",
    "\\frac{9}{35}\\end{aligned}\n",
    "$$ Therefore,\n",
    "$\\displaystyle \\phi_4(x) = (x - B_4)\\phi_3(x) - C_4\\phi_2(x) = x^4 - \\frac{6}{7}x^2 + \\frac{3}{35}$\n",
    "\n",
    "We want to find\n",
    "$P_4(x) = a_0 \\phi_0(x) + a_1 \\phi_1(x) + a_2 \\phi_2(x) + a_3\\phi_3(x) + a_4\\phi_4(x)$\n",
    "with\n",
    "$\\displaystyle{a_j = \\frac{\\int_{-1}^{1} w(x) f(x) \\phi_j(x) ~dx}{\\int_{-1}^{1} w(x) \\phi_j^2(x) ~dx} }$\n",
    "\n",
    "\\$\n",
    "\\$\n",
    "\n",
    "$\\displaystyle{P_4(x) = \\frac{1}{2} + \\frac{15}{16}\\left(x^2 - \\frac{1}{3} \\right) - \\frac{105}{128}\\left(x^4 - \\frac{6}{7}x^2 + \\frac{3}{35}\\right) = -\\frac{105 x^4}{128} + \\frac{105 x^2}{64} + \\frac{15}{128} }$\n",
    "\n",
    "The figure below, shows a comparison between $f(x) = |x|$ and $P_4(x)$.\n",
    "\n",
    "<span class=\"image\">Comparison between $f(x) = |x|$ and $P_4(x)$</span>\n",
    "\n",
    "## Summary: Least Squares Approximation - Orthogonal functions\n",
    "\n",
    "**Keywords:**Linear independent functions, Orthogonal functions,\n",
    "Gram-Schmidt process, normal equations\n",
    "\n",
    "**Intended learning outcomes (ILOs):**\n",
    "\n",
    "1.  Describe the disadvantages of least squares polynomial approximation\n",
    "\n",
    "2.  Define linear independent functions\n",
    "\n",
    "3.  Define weight function\n",
    "\n",
    "4.  Outline the uses of weight functions in least squares approximation\n",
    "\n",
    "5.  Define orthogonal and orthonormal set of functions\n",
    "    $\\int_{a}^b w(x) \\phi_k(x)\\phi_j(x) = \\left\\{ \\begin{array}{ll}  0,&\\;\\;\\;\\;\\;j \\neq k \\\\  \\alpha_j > 0,&\\;\\;\\;\\;\\;j = k \\end{array} \\right.$\n",
    "\n",
    "6.  Derive normal equations for least squares approximation using\n",
    "    orthogonal functions with weight function $f(x)$.\n",
    "    $\\int_a^b w(x)f(x)\\phi_j(x) dx = a_j \\int_a^b w(x) \\phi_j^2(x) dx,\\;\\; \\text{where}\\;\\; a_j = \\frac{1}{\\alpha_j}\\int_{a}^b w(x)f(x) \\phi_j(x) dx$\n",
    "\n",
    "7.  Use the Gram-Schmidt process to construct orthogonal polynomials\n",
    "    with respect to a weight function $w(x)$.  \n",
    "    The set of polynomials $\\{\\phi_0,\\phi_1,\\ldots,\\phi_n \\}$ defined in\n",
    "    the following way, is orthogonal on $[a,b]$ with respect to the\n",
    "    weight function $w(x)$. $$\\begin{aligned}\n",
    "    \\phi_0(x) &= 1,\\;\\;\\; \\phi_1(x) = x - B_1,\\;\\;\n",
    "     B_1 = \\frac{\\displaystyle{\\int_a^b x w(x) \\phi_0^2~dx }}{\\displaystyle{\\int_a^b w(x) \\phi_0^2~dx}},\\\\\n",
    "    \\phi_k & = (x-B_k)\\phi_{k-1} - C_k\\phi_{k-2},\\;\\;\\;\\text{when}\\;\\; k \\geq 2, \\\\\n",
    "       B_k & = \\frac{\\displaystyle{\\int_a^b x w(x) \\phi_{k-1}^2~dx }}{\\displaystyle{\\int_a^b w(x) \\phi_{k-1}^2~dx}},\\;\\;C_k  = \\frac{\\displaystyle{\\int_a^b x w(x) \\phi_{k-1}\\phi_{k-2}~dx }}{\\displaystyle{\\int_a^b w(x) \\phi_{k-2}^2~dx}}\\end{aligned}$$\n",
    "\n",
    "8.  Use orthogonal polynomials in least squares approximation of\n",
    "    functions.\n",
    "\n",
    "## Practice Problems\n",
    "\n",
    "1.  Use the Gram-Schmidt process to construct\n",
    "    $\\phi_0(x), \\phi_1(x), \\phi_2(x)$ and $\\phi_3(x)$ for the following\n",
    "    intervals\n",
    "\n",
    "    (a)   \\[0,1\\]                    (b)   \\[0,2\\]                     (c)   \\[1,3\\]\n",
    "\n",
    "2.  Use the results from 1. above to find the least squares polynomials\n",
    "    of degree two that approximates the following functions on the given\n",
    "    intervals.\n",
    "\n",
    "    1.  $f(x) = x^2 + 3x + 2,\\;\\;\\;\\;\\;\\;[0,1]$\n",
    "\n",
    "    2.  $f(x) = e^x,\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;[0,2]$\n",
    "\n",
    "    3.  $f(x) = \\frac{1}{x},\\;\\;\\;\\;\\;\\;\\;\\;\\;[1,3]$\n",
    "\n",
    "    4.  $f(x) = x\\ln(x),\\;\\;\\;\\;\\;\\;\\;\\;\\;[1,3]$\n",
    "\n",
    "3.  Use the Gram-Schmidt process to calculate $L_1, L_2, L_3$, where\n",
    "    $\\{L_0(x), L_1(x), L_2(x), L_3(x)\\}$ is an orthogonal set of\n",
    "    polynomials on $(0,\\infty)$ with respect to the weight functions\n",
    "    $w(x) = e^{-x}$ and $L_0(x) = 1$. The polynomials obtained from this\n",
    "    procedure are called the Laguerre polynomials.\n",
    "\n",
    "4.  Use the Laguerre polynomials calculated above to compute the least\n",
    "    squares polynomials of degree one, two and three on the interval\n",
    "    $(0,\\infty)$ with respect to the weight function $w(x) = e^{-x}$ for\n",
    "    the following functions\n",
    "\n",
    "    (a)   $f(x) = x^2$             (b)   $f(x) = e^{-x}$            (c)   $f(x) = x^3$\n",
    "\n",
    "5.  Use the Gram-Schmidt procedure to calculate $L_1,L_2$ and $L_3$\n",
    "    where $\\{L_0,L_1,L_2,L_3\\}$ is an orthogonal set of polynomials on\n",
    "    $(0,\\infty)$ with respect to the weight function $w(x) = e^{-x}$ and\n",
    "    $L_0(x) = 1$. Use the above polynomials, known as Laguerre\n",
    "    polynomials, to compute the least squares polynomial of degree 3 on\n",
    "    the interval $(0,\\infty)$ with respect to the weight function\n",
    "    $w(x) = e^{-x}$ for the function $e^{-2x}$.\n",
    "\n",
    "6.  Show that the functions $\\phi_n(x) = \\cos(nx)$, $n = 0,1,2,\\ldots,$\n",
    "    form an orthogonal family of functions on the interval $[-\\pi,\\pi]$\n",
    "    with weight function $w(x) = 1$.\n",
    "\n",
    "7.  The Legendre polynomials are given by $\\phi_0(x) = 1$,\n",
    "    $\\phi_1(x) = x$ and satisfy the recurrence relation\n",
    "    $\\phi_{n+1}(x) = \\frac{2n+1}{n+1} x \\phi_n(x) - \\frac{n}{n+1}\\phi_{n-1}(x),\\;\\;\\;\\;\\text{for}\\;\\; n \\geq 1.$\n",
    "\n",
    "    1.  Use the recurrence relation to find $\\phi_2(x), \\phi_3(x)$ and\n",
    "        $\\phi_4(x)$.\n",
    "\n",
    "    2.  Use Legendre polynomials to prove that the least squares\n",
    "        approximation of order $2$ for $f(x) = \\cos(\\pi x)$ on $[-1, 1]$\n",
    "        is $P_2(x) = \\frac{15}{2\\pi^2} - \\frac{45}{2\\pi^2}x^2$\n",
    "\n",
    "# Chebyshev Polynomials\n",
    "\n",
    "Chebyshev polynomials are a wonderful family polynomials that are\n",
    "orthogonal on $[-1,1]$ with respect to the weight function\n",
    "$w(x) = \\frac{1}{\\sqrt{1-x^2}}.$\n",
    "\n",
    "\\`\\`\\`{admonition} definition The set of polynomials defined by\n",
    "$T_n(x) = \\cos[n \\arccos x],\\;\\;\\;\\; n \\geq 0$ on $[-1,1]$ are called\n",
    "**Chebyshev Polynomials**\n",
    "\n",
    "The Chebyshev polynomials can be generated using the following recursive\n",
    "relation\n",
    "\n",
    "\n",
    "    To derive the recurrence formula we first note that\n",
    "    $T_0(x) = \\cos 0 = 1,\\;\\;\\;\\;\\text{and}\\;\\;\\; T_1(x) = \\cos(\\arccos x) = x.$\n",
    "    If we set $\\theta = \\arccos x$, then  \\begin{aligned}\n",
    "    T_n(x)& = \\cos(n\\theta),\\;\\;\\;\\;\\text{where}\\;\\; \\theta \\in [0,\\pi]\\\\\n",
    "    T_{n+1}(x)& = \\cos[(n+1)\\theta] = \\cos \\theta \\cos(n\\theta ) - \\sin \\theta \\sin(n\\theta)\\\\\n",
    "    T_{n-1}(x)& = \\cos[(n-1)\\theta] = \\cos \\theta \\cos(n\\theta ) + \\sin \\theta \\sin(n\\theta).\\end{aligned}\n",
    "    Adding the last two equations gives\n",
    "    $T_{n+1}(x) + T_{n-1}(x) = 2\\cos(n\\theta)\\cos \\theta.$ Substituting\n",
    "    back the variable $\\theta = \\arccos x$ gives  \\begin{aligned}\n",
    "    T_{n+1}(x)& = 2x\\cos(n\\arccos x) - T_{n-1}(x)\\\\\n",
    "              & = 2xT_{n}(x) - T_{n-1}(x).\\end{aligned}\n",
    "\n",
    "    Using the recurrence formula, successive Chebyshev polynomials can be\n",
    "    obtained as follows:  $$\\begin{aligned}\n",
    "    n &= 1;\\;\\;\\; T_{2}(x) = 2xT_1 - T_0 = 2x^2 - 1,\\\\\n",
    "    n &= 2;\\;\\;\\; T_{3}(x) = 2xT_2 - T_1 = 2x(2x^2 - 1)-x = 4x^3-3x,\\\\\n",
    "    n &= 3;\\;\\;\\; T_{4}(x) = 2xT_3 - T_2 = 2x(4x^3-3x)-(2x^2 - 1) = 8x^4-8x^2+1,\\end{aligned}$$\n",
    "    and so on.\n",
    "\n",
    "    Note that\n",
    "\n",
    "    1.  $T_n(x)$ is always a polynomial of degree $n$ with leading\n",
    "        coefficients $2^{n-1}$\n",
    "\n",
    "    2.  If $n$ is odd, $T_n(x)$ is an odd function\n",
    "\n",
    "    3.  If $n$ is even, $T_n(x)$ is an even function\n",
    "\n",
    "    We now show that the Chebyshev polynomials are orthogonal with respect\n",
    "    to the weight function\n",
    "    $w(x) = \\frac{1}{\\sqrt{1-x^2}}\\;\\;\\;\\;\\; \\text{in the interval }[-1,1]$\n",
    "\n",
    "     center\n",
    "\n",
    "\n",
    "    To prove the orthogonality property, we first consider the case when\n",
    "    $m \\neq n$. Using the fact that $T_n(x) = \\cos(n\\arccos(x))$ and\n",
    "    $T_m(x) = \\cos(m\\arccos(x))$, then,\n",
    "\n",
    "    $\\int_{-1}^{1}\\frac{T_m(x)T_n(x)}{\\sqrt{1-x^2}}dx = \\int_{-1}^{1}\\frac{ \\cos(m\\arccos(x))\\cos(n\\arccos(x))}{\\sqrt{1-x^2}}dx$\n",
    "    Let $\\theta = \\arccos(x)$, then $\\cos(\\theta) = x$ and hence $dx =\n",
    "    -\\sin(\\theta)d\\theta = -\\sqrt{1 - x^2}d\\theta$. Note that $x = -1$\n",
    "    corresponds to $\\theta = \\pi$ and $x = 1$ corresponds to $\\theta =\n",
    "    0$.\n",
    "\n",
    "    $d\\theta = -\\frac{dx}{\\sqrt{1 - x^2}}$ Then\n",
    "\n",
    "    $\\int_{-1}^{1}\\frac{T_m(x)T_n(x)}{\\sqrt{1-x^2}}dx =\n",
    "        -\\int_{\\pi}^{0}\\cos(n\\theta)\\cos(m\\theta)d\\theta = \\int_{0}^{\\pi}\\cos(n\\theta)\\cos(m\\theta)d\\theta$\n",
    "\n",
    "    When $m \\neq n$, we have $\\cos(n\\theta)\\cos(m\\theta) =\n",
    "    \\frac{\\cos((n+m)\\theta) + \\cos((n-m)\\theta)}{2}$ Therefore, we have\n",
    "    $\n",
    "     \\begin{aligned}\n",
    "    \\int_{-1}^{1}\\frac{T_m(x)T_n(x)}{\\sqrt{1-x^2}}dx &=\n",
    "    \\int_{0}^{\\pi}\\cos(n\\theta)\\cos(m\\theta)d\\theta\\\\\n",
    "    &=\\int_{0}^{\\pi}\\frac{\\cos((n+m)\\theta) + \\cos((n-m)\\theta)}{2}\\\\\n",
    "    &=\\int_{0}^{\\pi}\\frac{\\cos((n+m)\\theta)}{2} + \\int_{0}^{\\pi}\\frac{\\cos((n-m)\\theta)}{2}\\\\\n",
    "    &=\\left.\\frac{\\sin((n+m)\\theta)}{2(n+m)}\n",
    "    +\\frac{\\sin((n-m)\\theta)}{2(n-m)}\\right|_{0}^{\\pi}\\\\\n",
    "    &=\\frac{\\sin((n+m)\\pi)}{2(n+m)} +\\frac{\\sin((n-m)\\pi)}{2(n-m)}=0\\end{aligned}\n",
    "    $\n",
    "    Note that $m$ and $n$ are both integers, so the sum and difference of\n",
    "    $m$ and $n$ are integers too. We can therefore conclude that\n",
    "    $\\sin((n+m)\\pi) = \\sin((n-m)\\pi) = 0$.\n",
    "\n",
    "    When $m = n$, we have\n",
    "    $\n",
    "     \\begin{aligned}\n",
    "        \\int_{-1}^{1}\\frac{T_n(x)T_n(x)}{\\sqrt{1-x^2}}dx\n",
    "        &=\\int_{-1}^{1}\\frac{[T_n(x)]^2}{\\sqrt{1-x^2}}dx\\\\\n",
    "        &=-\\int_{\\pi}^{0}\\cos^2(n\\theta)d\\theta = \\int_{0}^{\\pi}\\cos^2(n\\theta)d\\theta\\end{aligned}\n",
    "    $\n",
    "    Note that $\\cos^2(n\\theta) =\n",
    "    \\frac{\\cos(2n\\theta) + 1}{2}$ and hence\n",
    "    $\n",
    "     \\begin{aligned}\n",
    "        \\int_{-1}^{1}\\frac{T_n(x)T_n(x)}{\\sqrt{1-x^2}}dx\n",
    "        &= \\int_{0}^{\\pi}\\cos^2(n\\theta)d\\theta\\\\\n",
    "        &= \\int_{0}^{\\pi}\\frac{\\cos(2n\\theta) + 1}{2}\\\\\n",
    "        &= \\left.\\frac{\\sin(2n\\theta)}{4n} + \\frac{\\theta}{2}\\right|_{0}^{\\pi} =\\frac{\\pi}{2}\\end{aligned}\n",
    "    $\n",
    "    When $m = n = 0$, we have\n",
    "    $\n",
    "     \\begin{aligned}\n",
    "        \\int_{-1}^{1}\\frac{T_0(x)T_0(x)}{\\sqrt{1-x^2}}dx\n",
    "        &=\\int_{-1}^{1}\\frac{[T_0(x)]^2}{\\sqrt{1-x^2}}dx\\\\\n",
    "        &=\\int_{-1}^{1}\\frac{dx}{\\sqrt{1-x^2}} =\\int_{0}^{\\pi}d\\theta = \\pi\n",
    "    \\end{aligned}\n",
    "    $\n",
    "\n",
    "    The next theorem concerns the zeros and extreme points of $T_n(x)$.\n",
    "\n",
    "     theorem\n",
    "    . The Chebyshev polynomials $T_n(x)$ of degree $n \\geq 1$ has the\n",
    "    following properties:\n",
    "\n",
    "    1.  $T_n(x)$ has simple roots in $[-1,1]$ at\n",
    "        $\\bar{x}_k = \\cos\\left(\\frac{2k-1}{2n}\\pi \\right),\\;\\;\\;\\;\\text{for each}\\;\\;k = 1,2,\\ldots,n$\n",
    "\n",
    "    $T_n(x)$ assumes an absolute extrema at\n",
    "    $\\tilde{x} = \\cos \\left( \\frac{\\pi k}{n}\\right)\\;\\;\\;\\;\\text{with}\\;\\; T_n(\\tilde{x}_k) = (-1)^k,\\;\\;\\text{for each}\\;\\; k = 0,1,\\ldots,n$\n",
    "\n",
    "\n",
    "     proof\n",
    "    *Proof.* Let $T_n(x) = \\cos (n \\arccos x) = \\cos n\\theta$. The roots are\n",
    "    obtained as\n",
    "    $\n",
    "     \\begin{aligned}\n",
    "    T_n(x) &= \\cos n\\theta = 0 \\Longrightarrow n\\theta = (2k-1)\\frac{\\pi}{2},\\;\\;\\;k = 1,2,3,\\ldots \\\\\n",
    "    \\theta & = (2k-1)\\frac{\\pi}{2n},\\\\\n",
    "    \\arccos x & = (2k-1)\\frac{\\pi}{2n},\\\\\n",
    "     \\bar{x}_k & = \\cos \\left[(2k-1)\\frac{\\pi}{2n} \\right],\\;\\; k = 1,2,\\ldots,n\\end{aligned}\n",
    "    $\n",
    "    are the zeros in $[-1,1]$.\n",
    "\n",
    "    To prove the second results, we find the derivative of $T_n(x)$ and\n",
    "    equate it to zero as follows:\n",
    "    $\n",
    "     \\begin{aligned}\n",
    "    \\frac{dT_n}{dx}&= \\frac{dT_n}{d\\theta} \\frac{d\\theta}{dx} = (-n \\sin n\\theta)\\left(- \\frac{1}{\\sqrt{1-x^2}}\\right) = 0,\\\\\n",
    "    \\therefore \\sin n\\theta & = 0,\\Longrightarrow  n \\theta = k \\pi,\\;\\;\\; k = 1,2,\\ldots,n-1,\\\\\n",
    "     \\arccos x & = \\frac{k \\pi}{n}\\\\\n",
    "     \\tilde{x}_k &= \\cos\\left( \\frac{k \\pi}{n}\\right)\\end{aligned}\n",
    "     $\n",
    "     Note\n",
    "    that $T_n(x)$ has degree $n$, thus $\\displaystyle \\frac{dT_n}{dx}$ has\n",
    "    degree $n-1$. Other possibilities for extrema of $T_n(x)$ occur at the\n",
    "    end points $[-1,1]$. That is at $\\tilde{x}_0 = 1$ and\n",
    "    $\\tilde{x}_n = -1$. Thus, for only $k = 0,1,2,\\ldots,n$, we have\n",
    "    $\n",
    "     \\begin{aligned}\n",
    "    T_n(\\tilde{x}_k)&= \\cos[n \\arccos \\tilde{x}_k]\\\\\n",
    "     & = \\cos\\left[n \\arccos \\cos\\left( \\frac{k \\pi}{n}\\right) \\right]\n",
    "     & = \\cos k \\pi = (-1)^k\\end{aligned}\n",
    "     $\n",
    "      Thus,\n",
    "    $T_n(\\tilde{x}_k) = (-1)^k$. ◻\n",
    "\n",
    "\n",
    "    ## Least Squares Approximation using Chebyshev Polynomials\n",
    "\n",
    "    Like any orthogonal set of polynomials, the Chebyshev polynomials can be\n",
    "    used to find least squares approximations of functions $f(x)$. Recall,\n",
    "    the that if $\\{\\phi_0,\\phi_1,\\ldots,\\phi_n\\}$ is an orthogonal set of\n",
    "    functions on an interval $[a,b]$ with respect to the weight function\n",
    "    $w(x)$, then the least squares approximation to $f(x)$ on $[a,b]$ with\n",
    "    respect to $w(x)$ is $P_n(x) = \\sum_{j = 0}^{n}a_j\\phi_j(x)$ where,\n",
    "    for each $j = 0,1,\\ldots,n$,\n",
    "    $\\alpha_j = \\frac{\\int_a^bw(x)\\phi_j(x)f(x)~dx}{\\int_a^b w(x)[\\phi_j(x)]^2} = \\frac{1}{\\alpha_j}\\int_a^b w(x)\\phi_j(x)f(x)~dx$\n",
    "\n",
    "    Thus, for Chebyshev polynomials, we set $w(x) = \\frac{1}{\\sqrt{1-x^2}}$,\n",
    "    $\\phi_j(x) = T_j(x)$ and $[a,b] = [-1,1]$. Then, using the orthogonal\n",
    "    property of Chebyshev polynomials, it can be seen that  \\begin{aligned}\n",
    "    \\alpha_0& = \\int_{-1}^{1}\\frac{T_0^2(x)}{\\sqrt{1-x^2}}~dx = \\pi,\\\\\n",
    "    \\alpha_j& = \\int_{-1}^{1}\\frac{T_j^2(x)}{\\sqrt{1-x^2}}~dx = \\frac{\\pi}{2},\\;\\;\\;\\;j = 1,2,\\ldots,n\\end{aligned}\n",
    "    Thus, we obtain the following formula the least squares approximation\n",
    "    using Chebyshev polynomials\n",
    "\n",
    "     center\n",
    "\n",
    "\n",
    "     example\n",
    "    Find a **quadratic** least-squares approximation of $f(x) = x^4$ using\n",
    "    Chebyshev polynomials.\n",
    "\n",
    "    **Solution:** We want to find\n",
    "    $P_2(x) = a_0T_0(x) + a_1T_1(x) + a_2T_2(x)$ where\n",
    "\n",
    "    $\n",
    "     \\begin{aligned}\n",
    "    a_0& = \\frac{1}{\\pi}\\int_{-1}^1 \\frac{x^4}{\\sqrt{1-x^2}}~dx = \\frac{3}{8}\\\\\n",
    "    a_1& = \\frac{2}{\\pi}\\int_{-1}^1 x\\frac{x^4}{\\sqrt{1-x^2}}~dx = 0\\\\\n",
    "    a_2& = \\frac{2}{\\pi}\\int_{-1}^1 (2x^2-1)\\frac{x^4}{\\sqrt{1-x^2}}~dx = \\frac{1}{2}\\end{aligned}\n",
    "    $\n",
    "    Thus,\n",
    "    $P_2(x) = \\frac{3}{8} + \\frac{1}{2}(2x^2 - 1)  = x^2  - \\frac{1}{8}$\n",
    "\n",
    "    The figure below, shows a comparison between $f(x) = x^4$ and $P_2(x)$.\n",
    "\n",
    "    ![Comparison between $f(x) = x^4$ and\n",
    "    $P_2(x)$](Othogonalcheb.eps){#figorthogonalcheb}\n",
    "\n",
    "\n",
    "    # Trigonometric Polynomial Approximation\n",
    "\n",
    "    It can be observed that for each integer $n > 0$, the set\n",
    "    $\\{\\phi_0,\\phi_1,\\ldots,\\phi_{2n-1} \\}$, where  \\begin{aligned}\n",
    "    \\phi_0(x) &= \\frac{1}{2},\\\\\n",
    "    \\phi_k(x) &= \\cos kx,\\;\\;\\;\\;\\;\\;\\;\\text{for each}\\;\\; k = 1,2,\\ldots,n\\\\\n",
    "    \\phi_{n+k}(x) &= \\sin kx,\\;\\;\\;\\;\\;\\;\\text{for each}\\;\\; k = 1,2,\\ldots,n-1\\end{aligned}\n",
    "    is orthogonal on $[-\\pi,\\pi]$ with respect to the weight $w(x) = 1$. The\n",
    "    orthogonality follows from the fact that for every integer $j$, the\n",
    "    integrals of $\\sin jx$ and $\\cos jx$ over $[-\\pi,\\pi]$ are 0, and the\n",
    "    products involving sine and cosine can be written as sums using the\n",
    "    following trigonometric identities\n",
    "    $\n",
    "     \\begin{aligned}\n",
    "    \\sin \\theta_1 \\sin \\theta_2 & = \\frac{1}{2}[\\cos(\\theta_1 - \\theta_2) - \\cos(\\theta_1 +\\theta_2)],\\\\\n",
    "    \\cos \\theta_1 \\cos \\theta_2 & = \\frac{1}{2}[\\cos(\\theta_1 - \\theta_2) + \\cos(\\theta_1 + \\theta_2)],\\\\\n",
    "    \\sin \\theta_1 \\cos \\theta_2 & = \\frac{1}{2}[\\sin(\\theta_1 - \\theta_2) + \\sin(\\theta_1 +\\theta_2)]\\end{aligned}\n",
    "    $\n",
    "     center\n",
    "\n",
    "\n",
    "     example\n",
    "    Determine the trigonometric polynomial from $S_n$ that approximates\n",
    "    $f(x) = |x|,;\\;\\;\\;\\;\\;\\text{for}\\;\\; -\\pi < x < \\pi$\n",
    "\n",
    "    **Solution:** We first determine the coefficients \n",
    "    $\n",
    "     \\begin{aligned}\n",
    "    a_0& = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} |x|~dx  = -\\frac{1}{\\pi} \\int_{-\\pi}^0 x~dx + \\frac{1}{\\pi}\\int_{0}^{\\pi} x~dx = \\frac{2}{\\pi}\\int_{0}^{\\pi}x~dx = \\pi,\\\\\n",
    "    a_k& = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}|x|\\cos kx~dx = \\frac{2}{\\pi}\\int_{0}^{\\pi} x\\cos kx~dx = \\frac{2}{\\pi k^2}[(-1)^k - 1],\\end{aligned}\n",
    "    $\n",
    "    for each $k = 1,2,\\ldots,n$ and\n",
    "    $b_k = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}|x|\\sin kx~dx = 0,\\;\\;\\;\\;\\text{for each}\\;\\;k=1,2,\\ldots,n-1$\n",
    "    The trigonometric polynomial that approximates $f(x) = |x|$ is\n",
    "    $S_n(x) = \\frac{\\pi}{2} + \\frac{2}{\\pi}\\sum_{k=1}^n \\frac{(-1)^k - 1}{k^2}\\cos kx$\n",
    "    Setting $n = 1$ and $n = 3$ gives\n",
    "\n",
    "    $S_1(x) = \\frac{\\pi}{2}  - \\frac{4}{\\pi}\\cos x,\\;\\;\\; S_3(x) = \\frac{\\pi}{2} - \\frac{4}{\\pi}\\cos x - \\frac{4}{9\\pi}\\cos 3x$\n",
    "    Figure [4](#triglsa1){reference-type=\"ref\" reference=\"triglsa1\"} shows a\n",
    "    comparison between $f(x)$ and $S_1(x)$ and $S_3(x)$\n",
    "\n",
    "    ![Comparison between $f(x) = |x|$ and $S_1(x)$ and\n",
    "    $S_3(x)$](triglsa1.eps){#triglsa1}\n",
    "\n",
    "\n",
    "     example\n",
    "    Let $f(x)$ be a function of period $2\\pi$ such that\n",
    "    $\\displaystyle{ f(x) = \\left\\{\\begin{array}{cc}\n",
    "    ~1~ , &~~~ -\\pi < x < 0\\\\\n",
    "    ~0~ , &~~~~ 0 < x < \\pi.\n",
    "    \\end{array} \\right. }$\n",
    "\n",
    "    1.  Show that the least squares trigonometric polynomial that\n",
    "        approximates $f(x)$ in the interval $-\\pi < x < \\pi$ is\n",
    "         $\n",
    "         \\begin{aligned}\n",
    "         S_n(x)& =  \\frac{1}{2} + \\sum_{k = 1}^{n}\\frac{1}{k\\pi} \\left[(-1)^k - 1 \\right]\\sin k x  \\\\ \n",
    "         &=  \\frac{1}{2} - \\frac{2}{\\pi}\\left[ \\sin x + \\frac{1}{3}\\sin 3x + \\frac{1}{5}\\sin 5x + \\ldots + \\frac{1}{(2n-1)}\\sin[(2n-1)x] \\right]\\end{aligned}\n",
    "         $\n",
    "        for $n = 1,2,\\ldots$\n",
    "\n",
    "    2.  By giving an appropriate value to $x$, show that as\n",
    "        $n \\rightarrow \\infty$.\n",
    "\n",
    "        $\\frac{\\pi}{4} = 1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7} + \\ldots$\n",
    "\n",
    "    **Solution:** The coefficients are defined as\n",
    "\n",
    "    1.  $ \\begin{aligned}\n",
    "        a_0 & = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f(x)~dx =\\frac{1}{\\pi}\\int_{-\\pi}^{0} 1~dx + \\frac{1}{\\pi}\\int_{0}^{\\pi}0~dx \\\\\n",
    "            & = \\frac{1}{\\pi}\\int_{-\\pi}^{0}~dx = \\left.\\frac{1}{\\pi} ~ x \\right|_{-\\pi}^{0} = 1 \\end{aligned}$\n",
    "\n",
    "        $\n",
    "         \\begin{aligned}\n",
    "        a_k & = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x)\\cos(kx)~dx\\\\\n",
    "            & = \\frac{1}{\\pi}\\int_{-\\pi}^{0}\\cos(kx)~dx + \\frac{1}{\\pi}\\int_{0}^{\\pi}0\\cdot \\cos(kx)~dx \\\\\n",
    "            & = \\frac{1}{\\pi}\\int_{-\\pi}^{0}\\cos(kx)~dx = \\left.\\frac{1}{\\pi k} \\sin(kx)\\right|_{-\\pi}^{0} = 0.\\end{aligned}\n",
    "        $\n",
    "         $\n",
    "         \\begin{aligned}\n",
    "        b_k & = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}f(x)\\sin(kx)~dx,\\\\\n",
    "            & = \\frac{1}{\\pi}\\int_{-\\pi}^{0}\\sin(kx)~dx  + \\frac{1}{\\pi}\\int_{0}^{\\pi}0\\cdot\\sin(kx)~dx \\\\  \n",
    "            & = \\frac{1}{\\pi}\\int_{-\\pi}^{0}\\sin(kx)~dx  = \\left. -\\frac{\\cos(kx)}{\\pi k}\\right|_{-\\pi}^{0} \\\\\n",
    "            & = -\\frac{1}{\\pi k}[1 - \\cos(k\\pi)] = \\frac{1}{\\pi k}[\\cos(k\\pi)-1] = \\frac{1}{k\\pi}\\left[(-1)^k - 1\\right]\\end{aligned}$\n",
    "        Thus,  $\\begin{aligned}\n",
    "        S_n(x)& = \\frac{1}{2} + \\sum_{k = 1}^{n-1}\\frac{1}{k\\pi} \\left[(-1)^k - 1 \\right]\\sin k x,\\\\\\end{aligned}$\n",
    "\n",
    "    2.  Suppose that\n",
    "        $\\displaystyle \\lim_{n \\rightarrow \\infty} S_n(x) = S(x)$.\n",
    "        $S(x) =\\frac{1}{2} - \\frac{2}{\\pi}\\left[\\sin x + \\frac{1}{3}\\sin 3x + \\frac{1}{5}\\sin 5x + \\frac{1}{7}\\sin 7x + \\ldots \\right]$\n",
    "        Set $x = \\frac{\\pi}{2}$ in $S(x)$. Note that when\n",
    "        $x = \\frac{\\pi}{2}$, $f(x) = 0$, thus\n",
    "\n",
    "        $ \\begin{aligned}\n",
    "        0 & = \\frac{1}{2} - \\frac{2}{\\pi}\\left[\\sin\\frac{\\pi}{2} + \\frac{1}{3}\\sin\\frac{3\\pi}{2} + \\frac{1}{5}\\sin \\frac{5\\pi}{2} + \\frac{1}{7}\\sin \\frac{7\\pi}{2} + \\ldots \\right]\\\\\n",
    "        0 & = \\frac{1}{2} - \\frac{2}{\\pi}\\left[1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7} + \\ldots \\right]\\\\\n",
    "        \\frac{\\pi}{4}&=1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7} + \\ldots  \\end{aligned}$\n",
    "\n",
    "    ```{note}\n",
    "\n",
    "    1.  If $f(x)$ is continuous at $x = a$, and the series\n",
    "        $S_{\\infty} = S(x)$ approximates $f(x)$, then $f(a) = S(a)$.\n",
    "\n",
    "    2.  If $f(x)$ is discontinuous at $x = a$, then the series converges to\n",
    "        a value that is halfway between the two possible values that is\n",
    "        $S(a) = \\frac{1}{2}(f(a^{-} + a^{+}))$\n",
    "\n",
    "# Summary: Least Squares Approximation using Trigonometric polynomials\n",
    "\n",
    "**Keywords:**Orthogonal trigonometric functions, Fourier Series\n",
    "\n",
    "**Intended learning outcomes (ILOs):**\n",
    "\n",
    "1.  Prove that the set\n",
    "    $\\mathcal{T}_n(x) = \\left\\{\\phi_0, \\phi_1,\\ldots, \\phi_{2n-1} \\right\\}$\n",
    "    with\n",
    "    $\\phi_0 = \\frac{1}{2},\\;\\; \\phi_k(x) = \\cos(k x), k = 0,1,\\ldots,n,\\;\\; \\phi_{k+n}(x) = \\sin(k x),\\;\\;k = 1,2,\\ldots,n-1$\n",
    "    is orthogonal on $[-\\pi,\\pi]$ with respect to $w(x) = 1$.\n",
    "\n",
    "2.  Describe the continuous least squares approximation using\n",
    "    trigonometric polynomial functions\n",
    "    $\\begin{aligned}  S_n(x)&= \\frac{a_0}{2} + a_n \\cos (n x) + \\sum_{k=1}^{n-1} \\left[ a_k \\cos (kx) + b_k \\sin(kx)\\right],\\\\  a_k &= \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\cos (kx)~dx,\\;\\;\\;k = 0,1,\\ldots,n,\\\\  b_k &= \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\sin (kx)~dx,\\;\\;\\;k = 1,2,\\ldots,n-1,\\\\  \\end{aligned}$\n",
    "\n",
    "3.  Describe how the Fourier series of a function $f(x)$ is obtained\n",
    "    from the continuous least squares approximation using\n",
    "    $\\mathcal{T}_n(x)$\n",
    "\n",
    "4.  Apply the trigonometric polynomial least squares approximation to\n",
    "    approximate a given function.\n",
    "\n",
    "5.  *Compare approximate solution obtained using trigonometric\n",
    "    polynomial least squares approximation with the exact value of the\n",
    "    function (Matlab exercise)*\n",
    "\n",
    "# Practice Problems\n",
    "\n",
    "1.  Find the continuous least squares trigonometric polynomial $S_3(x)$\n",
    "    for $f(x) = x$ on $[-\\pi,\\pi]$.\n",
    "\n",
    "2.  Find the continuous least-squares trigonometric polynomial $S_2(x)$\n",
    "    for $f(x) = x^2$ on $[-\\pi,\\pi]$.\n",
    "\n",
    "3.  Find the general general continuous least-squares trigonometric\n",
    "    polynomial $S_n(x)$ for the following functions on the interval\n",
    "    $[-\\pi,\\pi]$\n",
    "\n",
    "    1.  $f(x) = \\left\\{\\begin{array}{rl} -1,&\\;\\;\\; \\text{if},\\;\\; -\\pi < x < 0,\\\\ 1,&\\;\\;\\; \\text{if},\\;\\; 0 \\leq x \\leq \\pi. \\end{array} \\right.$\n",
    "\n",
    "    2.  $f(x) = \\left\\{\\begin{array}{rl} 0,&\\;\\;\\; \\text{if},\\;\\; -\\pi < x < 0,\\\\ 1,&\\;\\;\\; \\text{if},\\;\\; 0 \\leq x \\leq \\pi. \\end{array} \\right.$\n",
    "\n",
    "    3.  $\\displaystyle{ f(x) = \\left\\{\\begin{array}{cc} 0~ , &~~~ -\\pi < x < 0\\\\ x~ , &~~~~~~ 0 < x < \\pi \\end{array} \\right. }$\n",
    "\n",
    "    4.  $\\displaystyle{ f(x) = 1 + x }$           on $[-\\pi,\\pi]$ with\n",
    "        period $2\\pi$\n",
    "\n",
    "4.  Let $f(x)$ be a function of period $2\\pi$ such that\n",
    "    $\\displaystyle{ f(x) = \\left\\{\\begin{array}{cc} ~1~ , &~~~ -\\pi < x < 0\\\\ ~0~ , &~~~~ 0 < x < \\pi. \\end{array} \\right. }$\n",
    "\n",
    "    1.  Show that the continuous least-squares trigonometric polynomial\n",
    "        for $f(x)$ in the interval $-\\pi < x < \\pi$ is\n",
    "        $S(x) = \\frac{1}{2} - \\frac{2}{\\pi}\\left[ \\sin x + \\frac{1}{3}\\sin 3x + \\frac{1}{5}\\sin 5x + \\ldots \\right]$\n",
    "        as $n \\rightarrow \\infty$\n",
    "\n",
    "    2.  By giving an appropriate value to $x$, show that\n",
    "        $\\frac{\\pi}{4} = 1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7} + \\ldots$\n",
    "\n",
    "5.  Let $f(x)$ be a function of period $2\\pi$ such that\n",
    "    $\\displaystyle{ f(x) = \\left\\{\\begin{array}{cc} ~0~ , &~~~ -\\pi < x < 0\\\\ ~x~ , &~~~~ 0 < x < \\pi. \\end{array} \\right. }$\n",
    "\n",
    "    1.  Show that the continuous least-squares trigonometric polynomial\n",
    "        for $f(x)$ in the interval $-\\pi < x < \\pi$ is\n",
    "        $\\begin{aligned} S(x) = \\frac{\\pi}{4} & - \\frac{2}{\\pi}\\left[\\cos x + \\frac{1}{3^2}\\cos 3x + \\frac{1}{5^2}\\cos 5x + \\ldots \\right]\\\\  & + \\left[\\sin x - \\frac{1}{2}\\sin 2x + \\frac{1}{3}\\sin 3x - \\ldots \\right]\\end{aligned}$\n",
    "        as $n \\rightarrow \\infty$\n",
    "\n",
    "    2.  By giving an appropriate value to $x$, show that\n",
    "\n",
    "        1.  $\\displaystyle{ \\frac{\\pi}{4} = 1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7} + \\ldots}$\n",
    "\n",
    "        2.  $\\displaystyle{\\frac{\\pi^2}{8} = 1 + \\frac{1}{3^2} + \\frac{1}{5^2} + \\frac{1}{7^2} + \\ldots}$\n",
    "\n",
    "6.  Let $f(x)$ be a function of period $2\\pi$ such that\n",
    "    $f(x) = x \\;\\;\\;\\;\\text{over the interval}\\;\\; -\\pi < x < \\pi$\n",
    "\n",
    "    1.  Show that the Fourier series for $f(x)$ in the interval\n",
    "        $-\\pi < x < \\pi$ is\n",
    "        $S(x) = 2\\left[ \\sin x - \\frac{1}{2}\\sin 2x + \\frac{1}{3}\\sin 3x + \\ldots \\right]$\n",
    "        as $n \\rightarrow \\infty$\n",
    "\n",
    "    2.  By giving an appropriate value to $x$, show that\n",
    "        $\\frac{\\pi}{4} = 1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7} + \\ldots$\n",
    "\n",
    "7.  Let $f(x)$ be a function of period $2\\pi$ such that\n",
    "    $f(x) = x^2 \\;\\;\\;\\;\\text{over the interval}\\;\\; -\\pi \\leq x \\leq \\pi$\n",
    "\n",
    "    1.  Show that the continuous least-squares trigonometric polynomial\n",
    "        for $f(x)$ in the interval $-\\pi \\leq x \\leq \\pi$ is\n",
    "        $S(x) = \\frac{\\pi^2}{3} - 4\\left[ \\cos x - \\frac{1}{2^2}\\cos 2x + \\frac{1}{3^2}\\cos 3x - \\ldots \\right]$\n",
    "        as $n \\rightarrow \\infty$\n",
    "\n",
    "    2.  By giving an appropriate value to $x$, show that\n",
    "        $\\frac{\\pi^2}{6} = 1 + \\frac{1}{2^2} + \\frac{1}{3^2} + \\frac{1}{4^2} + \\ldots$\n",
    "\n",
    "# Answers\n",
    "\n",
    "1.  $S_3(x) = 2\\sin x - \\sin 2x$\n",
    "\n",
    "2.  $S_2(x) = \\frac{\\pi^2}{3} - 4\\cos x + \\cos 2x$\n",
    "\n",
    "3.  1.  $\\displaystyle{S_n(x) = \\frac{2}{\\pi} \\sum_{k=1}^{n-1} \\left(\\frac{1 - (-1)^k}{k} \\right)\\sin(kx)}$\n",
    "\n",
    "    2.  $\\displaystyle{S_n(x) = \\frac{1}{2} + \\frac{1}{\\pi} \\sum_{k=1}^{n-1} \\left(\\frac{1 - (-1)^k}{k} \\right)\\sin(kx)}$\n",
    "\n",
    "    3.  $\\displaystyle{ \\frac{\\pi}{4} + \\sum_{k=1}^{n} \\frac{(-1)^k - 1}{\\pi k^2}\\cos k x + \\sum_{k=1}^{n-1}+\\frac{(-1)^{k+1}}{k}\\sin kx}$\n",
    "\n",
    "    4.  $\\displaystyle{1 + \\sum_{k = 1}^{n-1} \\frac{2(-1)^{k+1}}{k} \\sin k x}$"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
